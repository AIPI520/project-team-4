{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder,OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression, f_classif, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "path = Path('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a Pandas dataframe\n",
    "df = pd.read_parquet(path='./outage_data.parquet', engine='pyarrow')\n",
    "\n",
    "# Remove duplicate entries in 2019\n",
    "# Remove all rows with SimStartDate after 2019-01-01 and event_type == 'thunderstorm'\n",
    "df = df.loc[~((df['SimStartDate'] > '2019-01-01') & (df['event_type'] == 'thunderstorm'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['point_ewkt'], inplace = True)\n",
    "\n",
    "# apply ordinal encoding to 'poly_ewkt', 'point_ewkt', 'event_type' columns\n",
    "non_numerical_columns = ['poly_ewkt', 'event_type']\n",
    "\n",
    "for column in non_numerical_columns:\n",
    "    df[column] = df[column].astype('category')\n",
    "    df[column] = df[column].cat.codes\n",
    "\n",
    "# Convert datetime columns to separate columns for year, month, day, hour, minute, second\n",
    "datetime_features = list(df.select_dtypes(include = \"datetime64[ns, UTC]\").columns)\n",
    "for i in datetime_features:\n",
    "    df[i+\"_year\"] = df[i].dt.year\n",
    "    df[i+\"_month\"] = df[i].dt.month\n",
    "    df[i+\"_day\"] = df[i].dt.day\n",
    "    df[i+\"_hour\"] = df[i].dt.hour\n",
    "\n",
    "# Use all data after Nov 1, 2018 (15 storms) as test set\n",
    "test_df = df.loc[df['SimStartDate'] >= '2018-11-01']\n",
    "train_df = df.loc[df['SimStartDate'] < '2018-11-01']\n",
    "\n",
    "# Drop columns that are not needed for training\n",
    "train_df = train_df.drop(datetime_features, axis=1)\n",
    "test_df = test_df.drop(datetime_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Droplist - 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "droplist = pickle.load(open('drop_list.pkl', 'rb'))\n",
    "train_df.drop(columns = droplist, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = df['outage_count']\n",
    "# cm_df = df.drop(['outage_count'], axis=1)\n",
    "threshold = 0.3\n",
    "feature_list = []\n",
    "cm_df = train_df.copy()\n",
    "\n",
    "target = cm_df['outage_count']\n",
    "cm_df = cm_df.drop(['outage_count'], axis=1)\n",
    "\n",
    "count = 0\n",
    "for col in cm_df.columns:\n",
    "    # print(\"col: \", col, \"dtype: \", cm_df[col].dtype)\n",
    "    if cm_df[col].dtype != 'object' and cm_df[col].dtype != 'datetime64[ns]' and cm_df[col].dtype != 'datetime64[ns, UTC]':\n",
    "        count += 1\n",
    "        # print(cm_df[col].dtype)\n",
    "        label = cm_df[col]\n",
    "        corr = label.corr(target)\n",
    "        # print(corr)\n",
    "        if abs(corr) > threshold:\n",
    "            feature_list.append(col)\n",
    "            print(\"col: \", col, \"correlation: \", corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Assume you have your data in X (features) and y (target)\n",
    "# Split the data into training and testing sets\n",
    "X = train_df.drop(['outage_count'], axis=1)\n",
    "y = train_df['outage_count']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (important for regularization methods)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a Lasso regression model\n",
    "lasso = Lasso(alpha=0.01)  # You can adjust the alpha parameter to control the strength of regularization\n",
    "\n",
    "# Fit the model on the training data\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = np.where(lasso.coef_ != 0)[0]\n",
    "\n",
    "# Print or use the selected features\n",
    "print(\"Selected Features:\", selected_features)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = lasso.score(X_test_scaled, y_test)\n",
    "print(\"Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_list_index = selected_features.tolist()\n",
    "# X_train = X_train.iloc[:, feature_list_index]\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Standardize the features (important for regularization methods)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Create a Lasso regression model\n",
    "# lasso = Lasso(alpha=0.05)  # You can adjust the alpha parameter to control the strength of regularization\n",
    "\n",
    "# # Fit the model on the training data\n",
    "# lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Get the selected features\n",
    "# # selected_features = np.where(lasso.coef_ != 0)[0]\n",
    "\n",
    "# # Print or use the selected features\n",
    "# # print(\"Selected Features:\", selected_features)\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# accuracy = lasso.score(X_test_scaled, y_test)\n",
    "# print(\"Model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['event_type',\n",
       " '220_220 (max)_lambert_levels 0-40000-filtered_24-max_min',\n",
       " 'SimStartDate_year',\n",
       " 'SimStartDate_hour',\n",
       " 'outage_start_time_year',\n",
       " 'outage_start_time_hour',\n",
       " 'outage_end_time_year',\n",
       " 'outage_end_time_hour',\n",
       " 'weather_start_time_year',\n",
       " 'weather_start_time_hour',\n",
       " 'weather_end_time_year',\n",
       " 'weather_end_time_hour']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve categorical features\n",
    "def check_categorical(df, max_unique_values=10):\n",
    "    \"\"\"\n",
    "    This function checks each column in the given dataframe to ascertain if it is categorical.\n",
    "    It is based on the number of unique values and the data type of the column.\n",
    "\n",
    "    :param df: Pandas DataFrame containing the data to check\n",
    "    :param max_unique_values: The maximum number of unique values to consider a feature as categorical\n",
    "    :return: A dictionary with the column name as keys and a boolean indicating if it is categorical as values\n",
    "    \"\"\"\n",
    "    categorical = {}\n",
    "    for column in df.columns:\n",
    "        # Check if the data type is object or if there are few unique values relative to the size of the dataset\n",
    "        if df[column].dtype == 'object' or len(df[column].unique()) <= max_unique_values:\n",
    "            categorical[column] = True\n",
    "        else:\n",
    "            # Check for numeric columns that are actually categorical (like IDs or codes)\n",
    "            # Here, we'll consider an integer column with a low number of unique values as categorical\n",
    "            if df[column].dtype in ['int64', 'int32'] and df[column].nunique() < max_unique_values:\n",
    "                categorical[column] = True\n",
    "            else:\n",
    "                categorical[column] = False\n",
    "    return categorical\n",
    "\n",
    "categorical = check_categorical(train_df, max_unique_values=10)\n",
    "categorical = {k: v for k, v in categorical.items() if v}\n",
    "categorical_list = list(categorical.keys())\n",
    "categorical_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "X = train_df.drop(['outage_count'], axis=1)\n",
    "y = train_df['outage_count']\n",
    "\n",
    "X_categorical = X[categorical_list]\n",
    "### S0LUTION FUNCTION\n",
    "# ADD YOUR SOLUTION TO THE DESIGNATED AREA\n",
    "# DO NOT ALTER THE SURROUNDING CODE\n",
    "\n",
    "def cat_feature_selection(cat_data,y):\n",
    "    ### BEGIN SOLUTION ###\n",
    "    ftest = SelectKBest(score_func=chi2, k='all')\n",
    "    ftest.fit(cat_data, y)\n",
    "    fscores = pd.DataFrame(ftest.scores_)\n",
    "    dfcolumns = pd.DataFrame(cat_data.columns)\n",
    "    featureScores = pd.concat([dfcolumns, fscores], axis=1)\n",
    "    featureScores.columns = ['Feature', 'Score']\n",
    "    featureScores = featureScores.sort_values('Score', ascending=False)\n",
    "    return featureScores\n",
    "\n",
    "featureScores = cat_feature_selection(X_categorical,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SimStartDate_hour</td>\n",
       "      <td>306.204227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>outage_start_time_hour</td>\n",
       "      <td>306.204227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>outage_end_time_hour</td>\n",
       "      <td>306.204227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>weather_start_time_hour</td>\n",
       "      <td>306.204227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weather_end_time_hour</td>\n",
       "      <td>306.204227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>event_type</td>\n",
       "      <td>45.659868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220_220 (max)_lambert_levels 0-40000-filtered_...</td>\n",
       "      <td>12.939021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SimStartDate_year</td>\n",
       "      <td>0.032677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>outage_start_time_year</td>\n",
       "      <td>0.032677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>outage_end_time_year</td>\n",
       "      <td>0.032677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>weather_start_time_year</td>\n",
       "      <td>0.032677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>weather_end_time_year</td>\n",
       "      <td>0.032677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Feature       Score\n",
       "3                                   SimStartDate_hour  306.204227\n",
       "5                              outage_start_time_hour  306.204227\n",
       "7                                outage_end_time_hour  306.204227\n",
       "9                             weather_start_time_hour  306.204227\n",
       "11                              weather_end_time_hour  306.204227\n",
       "0                                          event_type   45.659868\n",
       "1   220_220 (max)_lambert_levels 0-40000-filtered_...   12.939021\n",
       "2                                   SimStartDate_year    0.032677\n",
       "4                              outage_start_time_year    0.032677\n",
       "6                                outage_end_time_year    0.032677\n",
       "8                             weather_start_time_year    0.032677\n",
       "10                              weather_end_time_year    0.032677"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useless_cat_faetures = featureScores[featureScores['Score'].isna()]['Feature'].tolist()\n",
    "useless_cat_faetures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(useless_cat_faetures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('drop_list_anova_cat.pkl', 'wb') as f:\n",
    "    pickle.dump(useless_cat_faetures, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pickle.load(open('drop_list_anova_cat.pkl', 'rb')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
