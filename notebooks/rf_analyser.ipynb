{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(path='..//outage_data.parquet', engine='pyarrow')\n",
    "\n",
    "# Remove duplicate entries in 2019\n",
    "# Remove all rows with SimStartDate after 2019-01-01 and event_type == 'thunderstorm'\n",
    "df = df.loc[~((df['SimStartDate'] > '2019-01-01') & (df['event_type'] == 'thunderstorm'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply ordinal encoding to 'poly_ewkt', 'point_ewkt', 'event_type' columns\n",
    "\n",
    "non_numerical_columns = ['poly_ewkt', 'event_type']\n",
    "\n",
    "enconder = OrdinalEncoder()\n",
    "\n",
    "# Drop point_ewkt column (already in lat and lon columns)\n",
    "df = df.drop(columns=['point_ewkt'])\n",
    "\n",
    "enconder.fit(df[non_numerical_columns])\n",
    "df[non_numerical_columns] = enconder.transform(df[non_numerical_columns])\n",
    "\n",
    "#df[['poly_ewkt', 'point_ewkt', 'event_type']].head()\n",
    "#df['event_type'].value_counts()\n",
    "\n",
    "# Convert datetime columns to separate columns for year, month, day, hour, minute, second\n",
    "df['SimStartYear'] = df['SimStartDate'].dt.year\n",
    "df['SimStartMonth'] = df['SimStartDate'].dt.month\n",
    "df['SimStartDay'] = df['SimStartDate'].dt.day\n",
    "df['SimStartHour'] = df['SimStartDate'].dt.hour\n",
    "\n",
    "df['outage_start_year'] = df['outage_start_time'].dt.year\n",
    "df['outage_start_month'] = df['outage_start_time'].dt.month\n",
    "df['outage_start_day'] = df['outage_start_time'].dt.day\n",
    "df['outage_start_hour'] = df['outage_start_time'].dt.hour\n",
    "\n",
    "df['outage_end_year'] = df['outage_end_time'].dt.year\n",
    "df['outage_end_month'] = df['outage_end_time'].dt.month\n",
    "df['outage_end_day'] = df['outage_end_time'].dt.day\n",
    "df['outage_end_hour'] = df['outage_end_time'].dt.hour\n",
    "\n",
    "df['weather_start_year'] = df['weather_start_time'].dt.year\n",
    "df['weather_start_month'] = df['weather_start_time'].dt.month\n",
    "df['weather_start_day'] = df['weather_start_time'].dt.day\n",
    "df['weather_start_hour'] = df['weather_start_time'].dt.hour\n",
    "\n",
    "df['weather_end_year'] = df['weather_end_time'].dt.year\n",
    "df['weather_end_month'] = df['weather_end_time'].dt.month\n",
    "df['weather_end_day'] = df['weather_end_time'].dt.day\n",
    "df['weather_end_hour'] = df['weather_end_time'].dt.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all data after Nov 1, 2018 (15 storms) as test set\n",
    "test_df = df.loc[df['SimStartDate'] >= '2018-11-01']\n",
    "train_df = df.loc[df['SimStartDate'] < '2018-11-01']\n",
    "\n",
    "# Drop columns that are not needed for training\n",
    "train_df = train_df.drop(['SimStartDate', 'outage_start_time', 'outage_end_time', 'weather_start_time', 'weather_end_time'], axis=1)\n",
    "test_df = test_df.drop(['SimStartDate', 'outage_start_time', 'outage_end_time', 'weather_start_time', 'weather_end_time'], axis=1)\n",
    "\n",
    "X = train_df.drop(['outage_count'], axis=1)\n",
    "y = train_df['outage_count']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,test_size=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'min_samples_leaf':[1,3,10],'n_estimators':[100,1000],\n",
    "#          'max_features':[0.1,0.5,1.],'max_samples':[0.5,None]}\n",
    "#\n",
    "#model = RandomForestRegressor()\n",
    "#grid_search = GridSearchCV(model,params,cv=3)\n",
    "#grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=None, min_samples_split=2, random_state=0)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "test_preds = rf_model.predict(X_test)\n",
    "test_acc = np.sum(test_preds==y_test)/len(y_test)\n",
    "print('Test set accuracy is {:.3f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_feature_importances(model,feat_names,num_to_show):\n",
    "    # Determine the relative importance of each feature using the random forest model\n",
    "    importances = model.feature_importances_\n",
    "    # Get an array of the indices that would sort \"importances\" in reverse order to get largest to smallest\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    ranked_feats = []\n",
    "    for i in range(len(indices)):\n",
    "        feat_name = feat_names[indices[i]]\n",
    "        ranked_feats.append(feat_name)\n",
    "    RF_ranking = pd.DataFrame()\n",
    "    RF_ranking['Feat Index'] = indices\n",
    "    RF_ranking['Feature'] = ranked_feats\n",
    "    RF_ranking['Importance'] = np.sort(importances)[::-1]\n",
    "    display(RF_ranking.iloc[:num_to_show,:])\n",
    "\n",
    "    # Plot the importance value for each feature\n",
    "    RF_ranking[:num_to_show][::-1].plot(x='Feature',y='Importance',kind='barh',figsize=(12,7),legend=False,title='RF Feature Importance')\n",
    "    plt.show()\n",
    "    return RF_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feats = 30\n",
    "ranking = calc_feature_importances(rf_model,X.columns,top_feats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
